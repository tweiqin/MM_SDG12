name: 2MM - Database Migration

on:
  workflow_dispatch:
    inputs:
      confirm:
        description: 'Type "yes" to confirm migration (SAFE: No Data Loss)'
        required: true
        default: 'no'

jobs:
  migrate:
    name: Run DB Migration
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout Code
      uses: actions/checkout@v4

    - name: Configure AWS Credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: "ap-southeast-1"

    - name: Execute Migration via SSM
      shell: bash
      env:
        DB_HOST: ${{ secrets.DB_HOST }}
        DB_USER: ${{ secrets.DB_USERNAME }}
        DB_PASS: ${{ secrets.DB_PASSWORD }}
        DB_NAME: "makanmystery_db"
      run: |
        if [[ "${{ github.event.inputs.confirm }}" != "yes" ]]; then
          echo "Please confirm by typing 'yes' in the workflow input."
          exit 1
        fi
        
        # Clean DB_HOST (remove port if present)
        # Often Secrets contain host:port, but mysql -h expects just host.
        DB_HOST_CLEAN=$(echo "$DB_HOST" | cut -d':' -f1)
        echo "Using DB Host: $DB_HOST_CLEAN (Port stripped if present)"

        echo ">>> Step 2: Finding Resources..."
        # 2a. Find ASG
        ASG_NAME=$(aws autoscaling describe-auto-scaling-groups --query "AutoScalingGroups[?contains(AutoScalingGroupName, '-web-asg')].AutoScalingGroupName" --output text | head -n 1)
        if [ -z "$ASG_NAME" ] || [ "$ASG_NAME" == "None" ]; then
           echo "Error: Could not find Web ASG."
           exit 1
        fi
        echo "   ASG: $ASG_NAME"
        
        # 2b. Find S3 Bucket (Tag Project=makanmystery)
        BUCKET_NAME=$(aws s3api list-buckets --query "Buckets[?contains(Name, 'assets')].[Name]" --output text | grep "makanmystery" | head -n 1)
        if [ -z "$BUCKET_NAME" ]; then
             # Fallback search if naming convention differs
             BUCKET_NAME=$(aws resourcegroupstaggingapi get-resources --tag-filters Key=Project,Values=makanmystery --resource-type-filters s3:bucket --query "ResourceTagMappingList[0].ResourceARN" --output text | cut -d':' -f6)
        fi
        
        if [ -z "$BUCKET_NAME" ] || [ "$BUCKET_NAME" == "None" ]; then
           echo "Error: Could not find Project S3 Bucket."
           exit 1
        fi
        echo "   Bucket: $BUCKET_NAME"

        echo ">>> Step 3: Finding Healthy Instance..."
        INSTANCE_ID=$(aws autoscaling describe-auto-scaling-groups --auto-scaling-group-names "$ASG_NAME" --query "AutoScalingGroups[0].Instances[?LifecycleState=='InService'].InstanceId | [0]" --output text)

        if [ "$INSTANCE_ID" == "None" ] || [ -z "$INSTANCE_ID" ]; then
           echo "Error: No instances found in ASG!"
           exit 1
        fi
        echo "   Target Instance: $INSTANCE_ID"

        echo ">>> Step 4: Sending Migration Command..."
        
        # WE WILL RUN THIS ON THE HOST, NOT IN DOCKER
        # Why? Because we need to download from S3 first.
        # The host (EC2) has IAM role to access S3. The container might not.
        # But the host needs mysql-client.
        
        # Scripts to run:
        # 1. Install mysql-client (if missing)
        # 2. Download SQL from S3
        # 3. Drop/Create DB
        # 4. Import
        
        # NOTE: We use single quotes for shell script content to avoid variable expansion by GH Actions, 
        # BUT we need to inject GH Secrets. So we use specific interpolation.
        
        CMD_SCRIPT="
        set -e;
        echo 'Installing dependencies...';
        sudo apt-get update -y;
        sudo apt-get install -y mysql-client awscli;
        
        echo 'Downloading schema.sql from s3://$BUCKET_NAME/db/schema.sql...';
        aws s3 cp s3://$BUCKET_NAME/db/schema.sql /tmp/migration.sql;
        
        echo 'Resetting Database...';
        mysql -h $DB_HOST_CLEAN -u $DB_USER -p'$DB_PASS' -e 'DROP DATABASE IF EXISTS $DB_NAME; CREATE DATABASE $DB_NAME;';
        
        echo 'Importing SQL...';
        mysql -h $DB_HOST_CLEAN -u $DB_USER -p'$DB_PASS' $DB_NAME < /tmp/migration.sql;
        
        rm /tmp/migration.sql;
        echo 'Migration Success.';
        "
        
        # Sanitize for JSON
        CMD_ONELINE=$(echo "$CMD_SCRIPT" | tr '\n' ' ')
        CMD_ESCAPED=${CMD_ONELINE//\"/\\\"}
        
        PARAMS_JSON="{\"commands\":[\"$CMD_ESCAPED\"]}"

        COMMAND_ID=$(aws ssm send-command \
          --instance-ids "$INSTANCE_ID" \
          --document-name "AWS-RunShellScript" \
          --parameters "$PARAMS_JSON" \
          --query "Command.CommandId" \
          --output text)

        echo "   Command Sent! ID: $COMMAND_ID"
        
        # Wait for command execution
        aws ssm wait command-executed --command-id "$COMMAND_ID" --instance-id "$INSTANCE_ID" || true

        # Fetch output
        echo ">>> Output:"
        aws ssm get-command-invocation --command-id "$COMMAND_ID" --instance-id "$INSTANCE_ID" --query "StandardOutputContent" --output text
        
        # Fetch errors
        ERRORS=$(aws ssm get-command-invocation --command-id "$COMMAND_ID" --instance-id "$INSTANCE_ID" --query "StandardErrorContent" --output text)
        if [ ! -z "$ERRORS" ]; then
           echo ">>> Errors (Check if critical):"
           echo "$ERRORS"
           
           STATUS=$(aws ssm get-command-invocation --command-id "$COMMAND_ID" --instance-id "$INSTANCE_ID" --query "Status" --output text)
           if [ "$STATUS" == "Failed" ]; then
              echo "Migration Failed."
              exit 1
           fi
        fi
        echo ">>> Migration Complete!"
